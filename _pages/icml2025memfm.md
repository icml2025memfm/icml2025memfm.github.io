---
layout: home
order: 1
permalink: /
title: MemFM Workshop 2025
# redirect_from: /index.html
desc_title: The Impact of Memorization on Trustworthy Foundation Models ‚Äì MemFM @ ICML 2025
description: Understanding unintended memorization is essential to building trustworthy foundation models.
social: true
---

<!-- <td style="text-align:center"><img src="assets/img/workshop-votes.png" height="170"></td> <br />
<td style="text-align:center"><a href="https://bit.ly/bugs-orals">Vote Best Oral</a> | <a href="https://bit.ly/bugs-posters">Vote Best Poster</a></td> <br /> -->

Foundation models are rapidly becoming integral to high-stakes domains such as healthcare, public safety, and education. As their influence grows, so does the need to ensure they are reliable, ethical, and secure. A growing body of research, however, reveals a critical concern: foundation models are prone to <b>unintended memorization</b>‚Äîthe recall of specific details or even entire samples from their training data.

This phenomenon poses serious risks, including <b>privacy violations, intellectual property infringement, and societal harm</b> when sensitive or proprietary information is leaked. While some degree of memorization is necessary for solving complex tasks, unintended memorization threatens the integrity and trustworthiness of these systems. Striking the right balance between performance and privacy remains an open challenge.

Currently, solutions to this issue are being pursued across disparate research communities and data modalities‚Äîoften in isolation. This fragmentation leads to duplicated efforts and missed opportunities for collaboration, even when the goals are aligned. The lack of integration across fields like machine learning security, data privacy, and AI ethics hampers progress toward meaningful solutions.

This workshop aims to bring together researchers and practitioners to explore <b>the causes, consequences, and mitigations of unintended memorization.</b> By bridging insights across domains, we seek to foster collaboration, share practical strategies, and explore new theoretical foundations for mitigating these risks. Ultimately, our goal is to enable the development of trustworthy foundation models that serve society without compromising privacy, intellectual property, or public trust.

<!-- **UPDATE**: fill out this form if you are interested in a post-workshop social: [https://forms.gle/XjeSVmyHnsp7EmLB6](https://forms.gle/XjeSVmyHnsp7EmLB6). -->

<!-- ### Schedule (Meeting Room 317A, 9 AM - 5 PM, July 29, 2023) -->
### Schedule (West Meeting Room 223-224, Sat 19 Jul)

**Link to ICML page: [https://icml.cc/virtual/2025/workshop/39996](https://icml.cc/virtual/2025/workshop/39996)**

**All posters will be presented in both poster sessions**

|----------------------|---------------------------------------------------------|---------------------------------------------------------------------------------------|
| Start Time<br>(GMT-7-08:25, Vancouver)  |  Session                                                 | Speaker(s)                                                                            |
|:---------------------|:--------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|
| 08:25 am | Opening Remarks                                                                            | Organizers                                                                            |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 08:30 am | **Poster Session** (incl. catered breakfast snacks)                                                                               | Paper Authors |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 09:30 am | **Invited Talk 1:** On Testing Memorization in AI: From Brute-Force Methods to Robust Statistical Tests | Reza Shokri |
| 10:00 am | **Contributed Talk 1:** Evaluating Memorization in Parameter-Efficient Fine-tuning | Sanghyun Hong |
| 10:15 am | **Contributed Talk 2:** Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs | Tahseen Rabbani |
| 10:30 am  | **Invited Talk 2:** What Copyright Can Learn From Memorization Measurements of Language Models | A. Feder Cooper |
| 11:00 am | **Contributed Talk 3:** A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective | Qing Qu |
| 11:15 am  | **Invited Talk 3:** Trade-offs in Data Memorization via Strong Data Processing Inequalities | Vitaly Feldman |
| 11:45 am | **Contributed Talk 4:** MAGIC: Diffusion Model Memorization Auditing via Generative Image Compression | Gunjan Dhanuka |
| 12:00 pm | Lunch Break | |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 01:30 pm | **Contributed Talk 5** How Can I Publish My LLM Benchmark Without Giving the True Answers Away? | Takashi Ishida |
| 01:45 pm | **Invited Talk 4:** Principled Approaches to Measuring Memorization | Kamalika Chaudhuri |
| 02:15 pm | **Contributed Talk 6:** Low Resource Reconstruction Attacks Through Benign Prompts | Sol Yarkoni |
| 02:30 pm | **Poster Session 2** (incl. coffee break at 03:00 pm) | Paper Authors |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 03:45 pm | **Invited Talk 5:** What Memorization Research Taught Me About Safety | Pratyush Maini |
| 04:20 pm | **Panel Discussion** | Amy Beth Cyphert, Casey Meehan, Om Thakkar |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 04:50 pm   | Closing Remarks | Organizers    | 

### Speakers 

<!-- ### Speakers (Tentative) -->

<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/chaudhuri.jpeg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/cooper.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/feldman.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://cseweb.ucsd.edu/~kamalika/">Kamalika Chaudhuri</a> <small> <br> Professor / Research Scientist <br> UCSD / Meta </small> </td>
    <td style="text-align:center"><a href="https://afedercooper.info/">A. Feder Cooper</a> <small> <br>Incoming Assistant Professor / Postdoctoral Researcher<br> Yale University / Microsoft </small> </td>
    <td style="text-align:center"><a href="https://vtaly.net/">Vitaly Feldman</a> <small> <br> Research Scientist <br> Apple </small> </td>
  </tr>
  <tr>
    <td></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/maini.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/shokri.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td></td>
    <td style="text-align:center"><a href="https://pratyushmaini.github.io/">Pratyush Maini</a> <small><br> PhD Student <br> Carnegie Mellon University </small></td>
    <td style="text-align:center"><a href="https://www.comp.nus.edu.sg/~reza/">Reza Shokri</a> <small> <br> Associate Professor <br> National University of Singapore	</small> </td>
  </tr>
</table>

### Panelists

<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/cyphert.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/casey.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/thakkar.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://www.law.wvu.edu/faculty-staff/faculty-information/amy-cyphert">Amy Beth Cyphert</a> <small> <br> Associate Professor <br> West Virginia University College of Law </small> </td>
    <td style="text-align:center"><a href="https://casey-meehan.github.io/">Casey Meehan</a> <small> <br>Research Engineer<br> OpenAI </small> </td>
    <td style="text-align:center"><a href="https://www.omthakkar.com/">Om Thakkar</a> <small> <br> Research Engineer <br> OpenAI </small> </td>
  </tr>
</table>

### Call for Papers

**We cordially invite submissions and participation in our ‚ÄúThe Impact of Memorization on Trustworthy Foundation Models‚Äù workshop that will be held on July 19th, 2025 at the Forty-Second International Conference on Machine Learning (ICML) 2025 in Vancouver, Canada.**

<!-- The submission deadline is **<s>September 29, 2023</s> October 6th, 2023, 23:59 AoE** and the submission link <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>. -->

#### Motivation and Topics

This workshop explores the emerging challenges of memorization in foundation models, focusing on its detection, mitigation, and broader implications. Examples of research areas include: 

* **Detection and Mitigation Methods for Memorization in Foundation Models:** As foundation models grow in complexity, identifying instances of unintended memorization becomes both more challenging and more essential. This topic focuses on techniques for detecting memorized content‚Äîsuch as membership inference attacks and data reconstruction‚Äîas well as mitigation strategies, including regularization, differential privacy, and training data filtering. The goal is to prevent sensitive or proprietary data from being inadvertently retained and surfaced by the model.

* **Theoretical Foundations of Memorization:** Understanding the root causes of memorization requires a solid theoretical framework. This topic delves into formal definitions of memorization, how it emerges in high-capacity models, and its relationship to model architecture, training dynamics, and data distribution. Theoretical insights help build principled approaches to controlling memorization without compromising generalization.

* **Relationships Between Memorization and Security, Privacy, and Safety:** Memorization touches multiple dimensions of trustworthiness in AI systems. This topic investigates how memorized content can be exploited in adversarial settings, pose privacy violations, or trigger unexpected model behavior. By examining these interdependencies, we can better align memorization analysis with broader goals in AI security and responsible deployment.

* **Implications of Memorization on Generalization in Foundation Models:** A central tension in machine learning is the trade-off between memorizing training data and generalization. This topic focuses on how memorization impacts model robustness and performance across domains, and whether memorization can sometimes act as a proxy for poor generalization. Discussions here will explore how to find a healthy balance between these competing forces.

* **Societal Impact and Ethical Aspects of Memorization:** When foundation models unintentionally memorize and reveal private, personal, or copyrighted information, the consequences can be profound. This topic addresses the ethical responsibilities of researchers and developers, the potential for harm to individuals and communities, and the broader implications for fairness, accountability, and trust in AI technologies.


We welcome submissions related to all aspects of memorization in foundation models, including but not limited to: 

*	Detection and Mitigation Methods for Memorization in Foundation Models
*	Theoretical Foundations of Memorization
*	Quantification of the Degree of Memorization
*	Relationships Between Memorization and Security, Privacy, and Safety
*	Implications of Memorization on Generalization in Foundation Models
*	Connecting Memorization Research Across Different Domains and Applications
*	Societal Impact and Ethical Aspects of Memorization
*	Legal Perspectives on Memorization and Intellectual Property

The workshop will employ a double-blind review process. Each submission will be evaluated based on the following criteria:

* Soundness of the methodology
* Relevance to the workshop
* Societal impacts

**We allow dual submissions with other recent workshops or conferences. The workshop is non-archival and will not have any official proceedings**. All accepted papers will be allocated either a poster presentation or a talk slot.
 
<!-- ### Call for Reviewers
Please fill out this [Google form](https://docs.google.com/forms/d/e/1FAIpQLSd3L9_o7vAZUSWjWMxi18jZHuIrBaafUBm6v1fTZQorK2o9Qw/viewform) if you are interested in reviewing for the workshop.

üèÜ **2 free ICML 2023 workshop registrations will be given as "Best Reviewer Awards"** üèÜ -->

### Important Dates

* **Submission deadline**: May 27th, 2025, 11:59 PM Anywhere on Earth (AoE) <s>May 20th, 2025, 11:59 PM Anywhere on Earth (AoE)</s>
* **Author notification**: June 9th, 2025
* **Camera-ready deadline**: July 13th, 2025 11:59 PM Anywhere on Earth (AoE) <s>June 30th, 2025 11:59 PM Anywhere on Earth (AoE)</s>
* **Workshop date**: Saturday, July 19th, Meeting Room 223-224 (Full-day Event)

### Submission Instructions
Papers should be submitted to [OpenReview](https://openreview.net/group?id=ICML.cc/2025/Workshop/MemFM)

Submitted papers should have up to **4 pages** (excluding references, acknowledgments, or appendices). Please use our adjusted <a href="assets/latex/icml2025memfm.zip">ICML submission template</a>.
Submissions must be anonymous following ICML double-blind reviewing guidelines, ICML Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows.

üèÜ **The best paper will be recognized with a Best Paper Award** üèÜ 

### Workshop Sponsors


<table style="width:100%; align: center; border: none; spacing: none">
  <tr> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://hessian.ai/"><img src="assets/img/icml2025/sponsors/hessian-ai-logo-color.webp" height="80"></a></td>    
    <td style="text-align:center; border: none; spacing: none"><a href="https://cispa.de/en"><img src="assets/img/icml2025/organizers/affiliations/cispa.png" height="80"></a></td>    
  </tr>
</table>

<!-- <table style="width:100%; border: none;">
<td style="text-align:center; border: none;"><a href="https://troj.ai/"><img src="assets/img/sponsor-troj-ai.png" height="55"></a></td>

<td style="text-align:center; border: none;"><a href="https://ml.umd.edu/"><img src="assets/img/sponsor-umd-cml.png" height="65"></a></td>

<td style="text-align:center; border: none;"><a href="https://www.google.org/"><img src="assets/img/sponsor-google.png" height="75"></a></td>
</table> -->

### Organizers 


<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/franziskaboenisch-square.jpg" height="150"  width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/adamdziedzic-square.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/dominikhintersdorf-square.png" height="150" width="150"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://cispa.de/en/people/c01frbo">Franziska Boenisch</a> <br> <small> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://adam-dziedzic.com/">Adam Dziedzic</a> <small> <br> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://d0mih.github.io/">Dominik Hintersdorf</a> <small> <br>German Research Center for AI & TU Darmstadt </small> </td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/lingjuanlv.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/niloofarmireshghallah-square.jpg" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/lukasstruppek-square.png" height="150" width="150"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://sites.google.com/view/lingjuan-lyu/home/">Lingjuan Lyu</a> <small> <br>Sony AI </small> </td>
    <td style="text-align:center"><a href="https://homes.cs.washington.edu/~niloofar//">Niloofar Mireshghallah</a> <small> <br>FAIR & CMU</small> </td>
    <td style="text-align:center"><a href="https://lukasstruppek.github.io/">Lukas Struppek</a> <small> <br>German Research Center for AI & TU Darmstadt</small> </td>
  </tr>
</table>



### Organizer affiliations

<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://cispa.de/en"><img src="assets/img/icml2025/organizers/affiliations/cispa.png" height="75"></a></td>    
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.dfki.de/en/web"><img src="assets/img/icml2025/organizers/affiliations/dfki.png" height="75"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.tu-darmstadt.de/index.en.jsp"><img src="assets/img/icml2025/organizers/affiliations/tuda.png" height="75"></a></td>  
  </tr>
</table>
<table style="width:100%; align: left; border: none; spacing: none">
  <tr> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://ai.sony"><img src="assets/img/icml2025/organizers/affiliations/sony_ai.png" height="50"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.cs.washington.edu"><img src="assets/img/icml2025/organizers/affiliations/university_washington.png" height="50"></a></td>    
  </tr>
</table>
