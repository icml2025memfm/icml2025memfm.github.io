---
layout: home
order: 1
permalink: /
title: MemFM Workshop 2025
# redirect_from: /index.html
desc_title: The Impact of Memorization on Trustworthy Foundation Models – MemFM @ ICML 2025
description: Understanding unintended memorization is essential to building trustworthy foundation models.
social: true
---

<!-- <td style="text-align:center"><img src="assets/img/workshop-votes.png" height="170"></td> <br />
<td style="text-align:center"><a href="https://bit.ly/bugs-orals">Vote Best Oral</a> | <a href="https://bit.ly/bugs-posters">Vote Best Poster</a></td> <br /> -->

Foundation models are rapidly becoming integral to high-stakes domains such as healthcare, public safety, and education. As their influence grows, so does the need to ensure they are reliable, ethical, and secure. A growing body of research, however, reveals a critical concern: foundation models are prone to <b>unintended memorization</b>—the recall of specific details or even entire samples from their training data.

This phenomenon poses serious risks, including <b>privacy violations, intellectual property infringement, and societal harm</b> when sensitive or proprietary information is leaked. While some degree of memorization is necessary for solving complex tasks, unintended memorization threatens the integrity and trustworthiness of these systems. Striking the right balance between performance and privacy remains an open challenge.

Currently, solutions to this issue are being pursued across disparate research communities and data modalities—often in isolation. This fragmentation leads to duplicated efforts and missed opportunities for collaboration, even when the goals are aligned. The lack of integration across fields like machine learning security, data privacy, and AI ethics hampers progress toward meaningful solutions.

This workshop aims to bring together researchers and practitioners to explore <b>the causes, consequences, and mitigations of unintended memorization.</b> By bridging insights across domains, we seek to foster collaboration, share practical strategies, and explore new theoretical foundations for mitigating these risks. Ultimately, our goal is to enable the development of trustworthy foundation models that serve society without compromising privacy, intellectual property, or public trust.

<!-- **UPDATE**: fill out this form if you are interested in a post-workshop social: [https://forms.gle/XjeSVmyHnsp7EmLB6](https://forms.gle/XjeSVmyHnsp7EmLB6). -->

<!-- ### Schedule (Meeting Room 317A, 9 AM - 5 PM, July 29, 2023) -->
### Schedule

<!-- ⭐ **Link to NeurIPS page: [https://neurips.cc/virtual/2023/workshop/66550](https://neurips.cc/virtual/2023/workshop/66550)** ⭐ -->
#### ⭐ **Coming Soon** ⭐


<!-- |----------------------|---------------------------------------------------------|---------------------------------------------------------------------------------------|
| Start Time (CST/GMT-06:00, New Orleans)  |  Session                                                 | Speaker(s)                                                                            |
|:---------------------|:--------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|
| 08:55 am | Opening Remarks                                                                            | Organizers                                                                            |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 09:00 am | **Invited Talk 1:** A Blessing in Disguise: Backdoor Attacks as Watermarks for Dataset Copyright | Yiming Li |
| 09:30 am | **Invited Talk 2:** Recent Advances in Backdoor Defense and Benchmark | Baoyuan Wu  |
| 10:00 am | Coffee Break                                                                           |  |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 10:30 am | **Invited Talk 3:** The difference between safety and security for watermarking                                                                                | Jonas Geiping |
| 11:00 am | **Oral 1:** Effective Backdoor Mitigation Depends on the Pre-training Objective | Sahil Verma, Gantavya Bhatt, Soumye Singhal, Arnav Das, Chirag Shah, John Dickerson, Jeff A Bilmes |
| 11:15 am  | **Invited Talk 4:** Universal jailbreak backdoors from poisoned human feedback | Florian Tramèr |
| 11:45 am | Lunch Break | |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 01:00 pm | **Oral 2:** VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models | Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho |
| 01:15 pm | **Oral 3:** The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline | Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi |
| 01:30 pm | **Invited talk 5:** Is this model mine? On stealing and defending machine learning models | Adam Dziedzic |
| 02:00 pm | **Invited talk 6**                                                                           | Ruoxi Jia |
| 02:30 pm | Coffee Break                                                                     |  |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 03:00 pm | **Poster Session**                                                                                | Paper Authors |
| 03:45 pm | **Oral 4:** Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection | Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin |
| 04:00 pm | **Oral 5:** BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models | Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li |
| 04:15 pm | **Invited Talk 7:** Decoding Backdoors in LLMs and Their Implications | Bo Li |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 04:45 pm | **Panel Discussion**                                                                     | Moderator: Eugene Bagdasaryan |
| 05:15 pm   | Closing Remarks                                                                        | Organizers    |  -->


### Speakers (TBD)

<!-- ### Speakers (Tentative) -->

<!-- <table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/pinyuchen-square.webp" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/zicokolter-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/sanmikoyejo-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://sites.google.com/site/pinyuchenpage/home">Pin-Yu Chen</a> <small> <br> Principal Research Scientist <br> IBM Research AI </small> </td>
    <td style="text-align:center"><a href="https://zicokolter.com/">Zico Kolter</a> <small> <br> Professor <br> Carnegie Mellon University </small> </td>
    <td style="text-align:center"><a href="https://cs.stanford.edu/~sanmi/">Sanmi Koyejo</a> <small><br> Assistant Professor <br> Stanford University </small></td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/dawnsong-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/ericwallace-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/ericwong-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://dawnsong.io/">Dawn Song</a> <small> <br> Professor <br> University of California, Berkeley	</small> </td>
    <td style="text-align:center"><a href="https://www.ericswallace.com/">Eric Wallace</a> <small> <br> Member of Technical Staff <br> OpenAI </small> </td>
    <td style="text-align:center"><a href="https://riceric22.github.io/">Eric Wong</a> <small> <br> Assistant Professor <br> University of Pennsylvania </small> </td>
  </tr>
</table> -->

### Panelists (TBD)

<!-- ### Panelists (Tentative) -->

<!-- 
<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/tatsunorihashimoto-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/adinawilliams-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/rexying-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://thashim.github.io/">Tatsunori Hashimoto</a> <small> <br>Assistant Professor <br> Stanford University </small> </td>
    <td style="text-align:center"><a href="https://ai.meta.com/people/1396973444287406/adina-williams/">Adina Williams</a> <small> <br>Research Scientist <br> Facebook AI Research </small></td>
    <td style="text-align:center"><a href="https://www.cs.yale.edu/homes/ying-rex/">Rex (Zhitao) Ying</a> <small> <br>Assistant Professor <br> Yale University </small> </td>
  </tr>
</table> -->

### Call for Papers

**We cordially invite submissions and participation in our “The Impact of Memorization on Trustworthy Foundation Models” workshop that will be held on July 18th or July 19th, 2025 at the Forty-Second International Conference on Machine Learning (ICML) 2025 in Vancouver, Canada.**

<!-- The submission deadline is **<s>September 29, 2023</s> October 6th, 2023, 23:59 AoE** and the submission link <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>. -->

#### Motivation and Topics

This workshop explores the emerging challenges of memorization in foundation models, focusing on its detection, mitigation, and broader implications. Examples of research areas include: 

* **Detection and Mitigation Methods for Memorization in Foundation Models:** As foundation models grow in complexity, identifying instances of unintended memorization becomes both more challenging and more essential. This topic focuses on techniques for detecting memorized content—such as membership inference attacks and data reconstruction—as well as mitigation strategies, including regularization, differential privacy, and training data filtering. The goal is to prevent sensitive or proprietary data from being inadvertently retained and surfaced by the model.

* **Theoretical Foundations of Memorization:** Understanding the root causes of memorization requires a solid theoretical framework. This topic delves into formal definitions of memorization, how it emerges in high-capacity models, and its relationship to model architecture, training dynamics, and data distribution. Theoretical insights help build principled approaches to controlling memorization without compromising generalization.

* **Relationships Between Memorization and Security, Privacy, and Safety:** Memorization touches multiple dimensions of trustworthiness in AI systems. This topic investigates how memorized content can be exploited in adversarial settings, pose privacy violations, or trigger unexpected model behavior. By examining these interdependencies, we can better align memorization analysis with broader goals in AI security and responsible deployment.

* **Implications of Memorization on Generalization in Foundation Models:** A central tension in machine learning is the trade-off between memorizing training data and generalization. This topic focuses on how memorization impacts model robustness and performance across domains, and whether memorization can sometimes act as a proxy for poor generalization. Discussions here will explore how to find a healthy balance between these competing forces.

* **Societal Impact and Ethical Aspects of Memorization:** When foundation models unintentionally memorize and reveal private, personal, or copyrighted information, the consequences can be profound. This topic addresses the ethical responsibilities of researchers and developers, the potential for harm to individuals and communities, and the broader implications for fairness, accountability, and trust in AI technologies.


We welcome submissions related to all aspects of memorization in foundation models, including but not limited to: 

*	Detection and Mitigation Methods for Memorization in Foundation Models
*	Theoretical Foundations of Memorization
*	Quantification of the Degree of Memorization
*	Relationships Between Memorization and Security, Privacy, and Safety
*	Implications of Memorization on Generalization in Foundation Models
*	Connecting Memorization Research Across Different Domains and Applications
*	Societal Impact and Ethical Aspects of Memorization
*	Legal Perspectives on Memorization and Intellectual Property

The workshop will employ a double-blind review process. Each submission will be evaluated based on the following criteria:

* Soundness of the methodology
* Relevance to the workshop
* Societal impacts

We only consider submissions that haven’t been published in any peer-reviewed venue, including ICML 2025 conference. **We allow dual submissions with other workshops or conferences. The workshop is non-archival and will not have any official proceedings**. All accepted papers will be allocated either a poster presentation or a talk slot.
 
<!-- ### Call for Reviewers
Please fill out this [Google form](https://docs.google.com/forms/d/e/1FAIpQLSd3L9_o7vAZUSWjWMxi18jZHuIrBaafUBm6v1fTZQorK2o9Qw/viewform) if you are interested in reviewing for the workshop.

🏆 **2 free ICML 2023 workshop registrations will be given as "Best Reviewer Awards"** 🏆 -->

### Important Dates

* **Submission deadline**: May 20th, 2025, 11:59 PM Anywhere on Earth (AoE)
* **Author notification**: June 9th, 2025
* **Camera-ready deadline**: June 30th, 2025 11:59 PM Anywhere on Earth (AoE)
* **Workshop date**: TBD (Full-day Event)

### Submission Instructions
Papers should be submitted to [OpenReview](https://openreview.net/group?id=ICML.cc/2025/Workshop/MemFM)

Submitted papers should have up to **4 pages** (excluding references, acknowledgments, or appendices). Please use our adjusted <a href="assets/latex/icml2025memfm.zip">ICML submission template</a>.
Submissions must be anonymous following ICML double-blind reviewing guidelines, ICML Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows.

### Workshop Sponsors

#### ⭐ **Please reach out if you would like to sponsor our workshop.** ⭐

<!-- <table style="width:100%; border: none;">
<td style="text-align:center; border: none;"><a href="https://troj.ai/"><img src="assets/img/sponsor-troj-ai.png" height="55"></a></td>

<td style="text-align:center; border: none;"><a href="https://ml.umd.edu/"><img src="assets/img/sponsor-umd-cml.png" height="65"></a></td>

<td style="text-align:center; border: none;"><a href="https://www.google.org/"><img src="assets/img/sponsor-google.png" height="75"></a></td>
</table> -->

### Organizers 


<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/franziskaboenisch-square.jpg" height="150"  width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/adamdziedzic-square.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/dominikhintersdorf-square.png" height="150" width="150"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://cispa.de/en/people/c01frbo">Franziska Boenisch</a> <br> <small> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://adam-dziedzic.com/">Adam Dziedzic</a> <small> <br> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://d0mih.github.io/">Dominik Hintersdorf</a> <small> <br>German Research Center for AI & TU Darmstadt </small> </td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/lingjuanlv.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/blank-square.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/lukasstruppek-square.png" height="150" width="150"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://sites.google.com/view/lingjuan-lyu/home/">Lingjuan Lyu</a> <small> <br>Sony AI </small> </td>
    <td style="text-align:center"><a href="https://homes.cs.washington.edu/~niloofar//">Niloofar Mireshghallah</a> <small> <br>University of Washington </small> </td>
    <td style="text-align:center"><a href="https://lukasstruppek.github.io/">Lukas Struppek</a> <small> <br>German Research Center for AI & TU Darmstadt</small> </td>
  </tr>
</table>



### Organizer affiliations

<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://cispa.de/en"><img src="assets/img/icml2025/organizers/affiliations/cispa.png" height="75"></a></td>    
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.dfki.de/en/web"><img src="assets/img/icml2025/organizers/affiliations/dfki.png" height="75"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.tu-darmstadt.de/index.en.jsp"><img src="assets/img/icml2025/organizers/affiliations/tuda.png" height="75"></a></td>  
  </tr>
</table>
<table style="width:100%; align: left; border: none; spacing: none">
  <tr> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://ai.sony"><img src="assets/img/icml2025/organizers/affiliations/sony_ai.png" height="50"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.cs.washington.edu"><img src="assets/img/icml2025/organizers/affiliations/university_washington.png" height="50"></a></td>    
  </tr>
</table>
